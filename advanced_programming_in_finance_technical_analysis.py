# -*- coding: utf-8 -*-
"""Advanced Programming in Finance_Technical Analysis.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1rxk1jE5hkquSC9qT39PusJt9Sw29_lNo

# Important Assumption

1) Markets are not 100% random\
2) History repeats\
3) Markets follow people's rational behavior\
4) The markets are 'perfect'

# Library
"""

import numpy as np
import pandas as pd

import io, os, sys

from google.colab import drive
drive.mount('/content/gdrive/')

import time
import datetime
from dateutil.parser import parse

import seaborn as sns

import matplotlib.pyplot as plt
# %matplotlib inline
from sklearn.decomposition import PCA

import math

from sklearn.preprocessing import MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler

import xgboost as xgb
from sklearn.metrics import accuracy_score

!pip install python-utils
!pip install mxnet

from mxnet import nd, autograd, gluon
from mxnet.gluon import nn, rnn
import mxnet as mx

import warnings
warnings.filterwarnings("ignore")

context = mx.cpu()
model_ctx = mx.cpu()
mx.random.seed(1719)

"""# 1. Data

Predict the price movements of Goldman Sachs (NYSE: GS). Use daily closing price from January 1st, 2010 to June 28, 2019.\
Need to understand what affects whether GS's stock price will move up or down.\
80% train data.\
Feature: \
- Correlated assets
- Technical indicators
- Fourier transforms
- Autoregressive Integrated Moving Average (ARIMA)
- Stacked autoencoders
- Deep Unsupervised learning for anomaly detection in options pricing

Important Steps: \
1) Perform statistical checks\
2) Create feature importance

## 1.1. Import Data
"""

def parser(x):
    return datetime.datetime.strptime(x,'%Y-%m-%d')

my_path = "/content/gdrive/My Drive/Advance Programming in Finance/"
data_path = os.path.join(my_path, 'GS.csv')

dataset = pd.read_csv(data_path, header = 0, parse_dates = [0], date_parser = parser)

dataset_ex_df = dataset[['Date', 'Close']]
dataset_ex_df.columns = ['Date', 'GS']
dataset_ex_df.head()

"""## 1.2. Visualize Data"""

dataset_ex_df.loc[int(len(dataset_ex_df)*0.8)]['Date']

plt.figure(figsize = (14, 5), dpi = 100)

plt.plot(dataset_ex_df['Date'], dataset_ex_df['GS'], label = 'Goldman Sachs stock')

plt.vlines(datetime.date(2017, 8, 4), 0, 270, linestyles = '--', colors = 'gray', label = 'Train/Test data cut-off')

plt.xlabel('Date')
plt.ylabel('USD')
plt.title('Figure 2: Goldman Sachs stock price')

plt.legend()
plt.show()

"""# 2. Correlated Assets

Use other assets as features, not only GS.\
Good understanding of the company, its lines of businesses, competitive landscape, dependencies, suppliers and client type, etc.\
- JPMorgan Chase (https://finance.yahoo.com/quote/JPM/history?p=JPM) and Morgan Stanley (https://finance.yahoo.com/quote/MS/history?p=MS) daily close price
- Global economy indices: 1-month London Interbank Offered Rate (LIBOR) (https://www.macrotrends.net/2518/1-month-libor-rate-historical-chart). LIBOR is the basic short-term rate of interest in the Eurodollar market and the rate to which many Eurodollar loans and deposits are tied. The rate at which banks offer to lend money to other banks.
- Daily volatility index of GS (https://fred.stlouisfed.org/series/VXGSCLS)
- Composite indices: NASDAQ (https://fred.stlouisfed.org/series/NASDAQCOM) and NYSE (from USA) (https://finance.yahoo.com/quote/%5ENYA/history?period1=1262278800&period2=1561827600&interval=1d&filter=history&frequency=1d), FTSE100 (UK) (https://www.sharecast.com/index/FTSE_100/prices), Nikkei225 (Japan) (https://finance.yahoo.com/quote/%5EN225/history?period1=1262278800&period2=1561827600&interval=1d&filter=history&frequency=1d) indices
- Currencies: USD JPY (https://www.investing.com/currencies/usd-jpy-historical-data), GBP USD (https://www.investing.com/currencies/gbp-usd-historical-data)

## 2.1. Import and Preprocess
"""

# JPMorgan Chase daily close price 

JPM_path = os.path.join(my_path, 'JPM.csv')
JPM_dataset = pd.read_csv(JPM_path, header = 0, parse_dates = [0], date_parser = parser, index_col = ['Date'])
JPM_data = JPM_dataset[['Close']]
JPM_data.columns = ['JPM']
JPM_data.head()

# Morgan Stanley daily close price 

MS_path = os.path.join(my_path, 'MS.csv')
MS_dataset = pd.read_csv(MS_path, header = 0, parse_dates = [0], date_parser = parser, index_col = ['Date'])
MS_data = MS_dataset[['Close']]
MS_data.columns = ['MS']
MS_data.head()

# 1-month London Interbank Offered Rate (LIBOR)

libor_path = os.path.join(my_path, 'one_month_LIBOR_rate.csv')
libor_dataset = pd.read_csv(libor_path, header = 0, parse_dates = [0], date_parser = parser, index_col = ['Date'])
libor_data = libor_dataset[['LIBOR']]
libor_data.columns = ['libor']
libor_data.head()

# Daily volatility index of GS

volatility_path = os.path.join(my_path, 'volatility_index_GS.csv')
volatility_dataset = pd.read_csv(volatility_path, header = 0, parse_dates = [0], date_parser = parser)
volatility_dataset.columns = ['Date', 'volatility_GS']
volatility_data = volatility_dataset.set_index('Date')
volatility_data.head()

# NASDAQ

NASDAQ_path = os.path.join(my_path, 'NASDAQ.csv')
NASDAQ_dataset = pd.read_csv(NASDAQ_path, header = 0, parse_dates = [0], date_parser = parser)
NASDAQ_dataset.columns = ['Date', 'NASDAQ']
NASDAQ_data = NASDAQ_dataset.set_index('Date')
NASDAQ_data.head()

# NYSE

NYSE_path = os.path.join(my_path, 'NYSE.csv')
NYSE_dataset = pd.read_csv(NYSE_path, header = 0, parse_dates = [0], date_parser = parser)
NYSE_data = NYSE_dataset[['Date', 'Close']]
NYSE_data.columns = ['Date', 'NYSE']
NYSE_data = NYSE_data.set_index('Date')
NYSE_data.head()

# FTSE100

FTSE100_path = os.path.join(my_path, 'FTSE100.csv')
FTSE100_dataset = pd.read_csv(FTSE100_path)

fdate = FTSE100_dataset[['Date']]
formated_date = []
for i in range(len(FTSE100_dataset)):
  formated_date.append(parse(fdate['Date'].values[i]).date().strftime('%Y-%m-%d'))
  
FTSE100_dataset[['Date']] = formated_date

FTSE100_data = FTSE100_dataset[['Date', 'Close']]
FTSE100_data.columns = ['Date', 'FTSE100']
FTSE100_data = FTSE100_data.set_index('Date')
FTSE100_data.head()

# Nikkei225

Nikkei225_path = os.path.join(my_path, 'Nikkei225.csv')
Nikkei225_dataset = pd.read_csv(Nikkei225_path, header = 0, parse_dates = [0], date_parser = parser)
Nikkei225_data = Nikkei225_dataset[['Date', 'Close']]
Nikkei225_data.columns = ['Date', 'Nikkei225']
Nikkei225_data = Nikkei225_data.set_index('Date')
Nikkei225_data.head()

# USD JPY

USDJPY_path = os.path.join(my_path, 'USD_JPY.csv')
USDJPY_dataset = pd.read_csv(USDJPY_path)

fdate = USDJPY_dataset[['Date']]
fdate
formated_date = []
for i in range(len(USDJPY_dataset)):
  formated_date.append(parse(fdate['Date'].values[i]).date().strftime('%Y-%m-%d'))
  
USDJPY_dataset[['Date']] = formated_date

USDJPY_data = USDJPY_dataset[['Date', 'Price']]
USDJPY_data.columns = ['Date', 'USDJPY']
USDJPY_data = USDJPY_data.set_index('Date')
USDJPY_data.head()

# GBP USD

GBPUSD_path = os.path.join(my_path, 'GBP_USD.csv')
GBPUSD_dataset = pd.read_csv(GBPUSD_path)

fdate = GBPUSD_dataset[['Date']]
fdate
formated_date = []
for i in range(len(GBPUSD_dataset)):
  formated_date.append(parse(fdate['Date'].values[i]).date().strftime('%Y-%m-%d'))
  
GBPUSD_dataset[['Date']] = formated_date

GBPUSD_data = GBPUSD_dataset[['Date', 'Price']]
GBPUSD_data.columns = ['Date', 'GBPUSD']
GBPUSD_data = GBPUSD_data.set_index('Date')
GBPUSD_data.head()

"""## 2.2. Merge"""

print(len(JPM_data), len(MS_data), len(libor_data), len(volatility_data), len(NASDAQ_data), len(NYSE_data), len(FTSE100_data), len(Nikkei225_data), len(USDJPY_data), len(GBPUSD_data))

corr_asset_data = pd.merge(libor_data, USDJPY_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, GBPUSD_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, NASDAQ_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, FTSE100_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, NYSE_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, MS_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, JPM_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, volatility_data, left_index = True, right_index = True)
corr_asset_data = pd.merge(corr_asset_data, Nikkei225_data, left_index = True, right_index = True)
corr_asset_data.head()

len(corr_asset_data)

"""# 3. Technical Indicators

A lot of investors follow technical indicators. The most popular indicators as independent features: 7 and 21 days moving average, exponential moving average, momentum, Bollinger bands, MACD.

## 3.1. Calculate
"""

dataset_TI_df = dataset_ex_df.copy()
dataset_TI_df.columns = ['Date', 'price']
n = len(dataset_TI_df)
dataset_TI_df.head()

ewma = pd.Series.ewm

def get_technical_indicators(dataset):
    # Create 7 and 21 days Moving Average
    dataset['ma7'] = dataset['price'].rolling(window = 7).mean()
    dataset['ma21'] = dataset['price'].rolling(window = 21).mean()
    
    # Create MACD
    dataset['26ema'] = ewma(dataset['price'], span = 26).mean()
    dataset['12ema'] = ewma(dataset['price'], span = 12).mean()
    dataset['MACD'] = (dataset['12ema']-dataset['26ema'])
    
    # Create Bollinger Bands
    dataset['20sd'] = dataset['price'].rolling(20).std()
    dataset['upper_band'] = dataset['ma21'] + (dataset['20sd']*2)
    dataset['lower_band'] = dataset['ma21'] - (dataset['20sd']*2)
    
    # Create Exponential moving average
    dataset['ema'] = dataset['price'].ewm(com = 0.5).mean()
    
    # Create Momentum
    dataset['momentum'] = dataset['price'] - 1
    dataset['log_momentum'] = np.log(dataset['momentum'])

get_technical_indicators(dataset_TI_df)
dataset_TI_df = dataset_TI_df[20:n]
dataset_TI_df.head()

"""## 3.2. Visualize"""

def plot_technical_indicators(dataset, last_days):
    plt.figure(figsize = (16, 10), dpi = 100)
    shape_0 = dataset.shape[0]
    xmacd_ = shape_0 - last_days
    
    dataset = dataset.iloc[- last_days:, :]
    x_ = range(3, dataset.shape[0])
    x_ = list(dataset.index)
    
    # Plot first subplot
    plt.subplot(2, 1, 1)
    plt.plot(dataset['ma7'], label = 'MA 7', color = 'g', linestyle = '--')
    plt.plot(dataset['price'], label = 'Closing Price', color='b')
    plt.plot(dataset['ma21'], label = 'MA 21', color = 'r', linestyle = '--')
    plt.plot(dataset['upper_band'], label = 'Upper Band', color = 'c')
    plt.plot(dataset['lower_band'], label = 'Lower Band', color = 'c')
    plt.fill_between(x_, dataset['lower_band'], dataset['upper_band'], alpha = 0.35)
    plt.title('Technical indicators for Goldman Sachs - last {} days.'.format(last_days))
    plt.ylabel('USD')
    plt.legend()

    # Plot second subplot
    plt.subplot(2, 1, 2)
    plt.title('MACD')
    plt.plot(dataset['MACD'], label = 'MACD', linestyle = '-.')
    plt.hlines(15, xmacd_, shape_0, colors = 'g', linestyles = '--')
    plt.hlines(-15, xmacd_, shape_0, colors = 'g', linestyles = '--')
    plt.plot(dataset['log_momentum'], label = 'Momentum', color = 'b', linestyle = '-')

    plt.legend()
    plt.show()

n_day = 500
plot_technical_indicators(dataset_TI_df, n_day)

"""# 4. Fourier Transforms for Trend Analysis

Fourier transforms take a function and create a series of sine waves (with different amplitudes and frames). When combined, these sine waves approximate the original function. Mathematically speaking, the transforms look like this:

$$G(f) = \int_{-\infty}^\infty g(t) e^{-i 2 \pi f t} dt$$

We will use Fourier transforms to extract global and local trends in the GS stock, and to also denoise it a little.

## 4.1. Calculate
"""

data_FT = dataset_ex_df.copy()

data_FT.head()

close_fft = np.fft.fft(np.asarray(data_FT['GS'].tolist()))
fft_df = pd.DataFrame({'fft':close_fft})
fft_df['absolute'] = fft_df['fft'].apply(lambda x: np.abs(x))
fft_df['angle'] = fft_df['fft'].apply(lambda x: np.angle(x))

fft_df.head()

len(fft_df)

"""## 4.2. Visualize"""

plt.figure(figsize = (14, 7), dpi = 100)

fft_list = np.asarray(fft_df['fft'].tolist())

for num_ in [3, 6, 9, 100]:
    fft_list_m10 = np.copy(fft_list)
    fft_list_m10[num_:-num_] = 0
    plt.plot(np.fft.ifft(fft_list_m10), label = 'Fourier transform with {} components'.format(num_))
    
plt.plot(data_FT['GS'],  label = 'Real')
plt.xlabel('Days')
plt.ylabel('USD')
plt.title('Figure 3: Goldman Sachs (close) stock prices & Fourier transforms')

plt.legend()
plt.show()

"""As you see in Figure 3 the more components from the Fourier transform we use the closer the approximation function is to the real stock price (the 100 components transform is almost identical to the original function - the red and the purple lines almost overlap). 

We ***use Fourier transforms for the purpose of extracting long- and short-term trends*** so we will use the transforms with 3, 6, and 9 components. 

You can ***infer that the transform with 3 components serves as the long term trend***.
"""

fft_list_m10 = np.copy(fft_list)
fft_list_m10[3:-3] = 0
fft_df['fft_3_component'] = np.fft.ifft(fft_list_m10)

fft_list_m10 = np.copy(fft_list)
fft_list_m10[6:-6] = 0
fft_df['fft_6_component'] = np.fft.ifft(fft_list_m10)

fft_list_m10 = np.copy(fft_list)
fft_list_m10[9:-9] = 0
fft_df['fft_9_component'] = np.fft.ifft(fft_list_m10)

fft_list_m10 = np.copy(fft_list)
fft_list_m10[100:-100] = 0
fft_df['fft_100_component'] = np.fft.ifft(fft_list_m10)

fourier_transform_data = fft_df[['fft', 'fft_3_component', 'fft_6_component', 'fft_9_component', 'fft_100_component']]
fourier_transform_data['Date'] = data_FT['Date']
fourier_transform_data = fourier_transform_data.set_index('Date')
fourier_transform_data.head()

"""# 5. ARIMA as a feature

ARIMA is a technique for predicting time series data. Use it as a technique to denoise the stock a little and to (possibly) extract some new patters or features.
"""

from statsmodels.tsa.arima_model import ARIMA
from pandas import DataFrame
from pandas import datetime

series = data_FT['GS']
model = ARIMA(series, order = (5, 1, 0))
model_fit = model.fit(disp = 0)
print(model_fit.summary())

# Autocorrelation Plot

from pandas.plotting import autocorrelation_plot

autocorrelation_plot(series)
plt.figure(figsize = (14, 10), dpi = 100)
plt.show()

from pandas import read_csv
from pandas import datetime
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error

X = series.values
size = int(len(X) * 0.6)
train, test = X[0:size], X[size:len(X)]
history = [x for x in train]

predictions = list()
for t in range(len(test)):
    model = ARIMA(history, order = (5,1,0))
    model_fit = model.fit(disp = 0)
    output = model_fit.forecast()
    yhat = output[0]
    predictions.append(yhat)
    obs = test[t]
    history.append(obs)

error = mean_squared_error(test, predictions)
print('Test MSE: %.3f' % error)

# Plot the predicted (from ARIMA) and real prices

plt.figure(figsize = (12, 6), dpi = 100)
plt.plot(test, label='Real')
plt.plot(predictions, color = 'red', label = 'Predicted')
plt.xlabel('Days')
plt.ylabel('USD')
plt.title('Figure 5: ARIMA model on GS stock')
plt.legend()
plt.show()

"""As we can see from Figure 5 ARIMA gives a very good approximation of the real stock price.

We will ***use the predicted price through ARIMA as an input feature into the LSTM*** because, as we mentioned before, we want to capture as many features and patterns about Goldman Sachs as possible. 

We go test MSE (mean squared error) of 9.916, which by itself is not a bad result (considering we do have a lot of test data), but still we will only use it as a feature in the LSTM.

# 6. Feature Engineering

## Merge all data

Merge all types of data (the correlated assets, technical indicators, Fourier)
"""

technical_data = dataset_TI_df.copy()
technical_data = technical_data.set_index('Date')
technical_data.head()

print(len(corr_asset_data), len(fourier_transform_data), len(technical_data))

VAE_data = pd.merge(fourier_transform_data, technical_data, left_index = True, right_index = True)
VAE_data = pd.merge(VAE_data, corr_asset_data, left_index = True, right_index = True)
VAE_data.head()

len(VAE_data)

"""## Feature importance with XGBoost

Having so many features we have to consider whether all of them are really indicative of the direction GS stock will take. For example, we included USD denominated LIBOR rates in the dataset because we think that changes in LIBOR might indicate changes in the economy, that, in turn, might indicate changes in the GS's stock behavior. But we need to test. There are many ways to test feature importance, but the one we will apply uses XGBoost, because it gives one of the best results in both classification and regression problems.
"""

def get_feature_importance_data(data_income, train_test_rate):
    data = data_income.copy()
    y = data['price']
    X = data.iloc[:, [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26]]
    
    train_samples = int(X.shape[0] * train_test_rate)
 
    X_train = X.iloc[:train_samples]
    X_test = X.iloc[train_samples:]

    y_train = y.iloc[:train_samples]
    y_test = y.iloc[train_samples:]
    
    return (X_train, y_train), (X_test, y_test)

# Get training and test data

(X_train_VAE, y_train_VAE), (X_test_VAE, y_test_VAE) = get_feature_importance_data(VAE_data, 0.8)
# (X_train_FI, y_train_FI), (X_test_FI, y_test_FI) = get_feature_importance_data(dataset_TI_df, 0.8)

X_train_VAE.head()
# X_train_FI.head()

y_train_VAE.head()
# y_train_FI.head()

gamma = 0.0
n_estimators = 150
base_score = 0.7
colsample_bytree = 1
learning_rate = 0.05

regressor = xgb.XGBRegressor(gamma = gamma, n_estimators = n_estimators, base_score = base_score, colsample_bytree = colsample_bytree, learning_rate = learning_rate)

xgbModel = regressor.fit(X_train_VAE,y_train_VAE, \
                         eval_set = [(X_train_VAE, y_train_VAE), (X_test_VAE, y_test_VAE)], \
                         verbose = False)
# xgbModel = regressor.fit(X_train_FI,y_train_FI, \
#                          eval_set = [(X_train_FI, y_train_FI), (X_test_FI, y_test_FI)], \
#                          verbose=False)

eval_result = regressor.evals_result()

training_rounds = range(len(eval_result['validation_0']['rmse']))

# Plot the training and validation errors in order to observe the training and check for overfitting

plt.scatter(x=training_rounds,y=eval_result['validation_0']['rmse'],label='Training Error')
plt.scatter(x=training_rounds,y=eval_result['validation_1']['rmse'],label='Validation Error')

plt.xlabel('Iterations')
plt.ylabel('RMSE')
plt.title('Training Vs Validation Error')

plt.legend()
plt.show()

fig = plt.figure(figsize = (8,8))
plt.xticks(rotation = 'vertical')
plt.bar([i for i in range(len(xgbModel.feature_importances_))], xgbModel.feature_importances_.tolist(), tick_label = X_test_VAE.columns)
# plt.bar([i for i in range(len(xgbModel.feature_importances_))], xgbModel.feature_importances_.tolist(), tick_label = X_test_FI.columns)
plt.title('Figure 6: Feature importance of the technical indicators.')
plt.show()

"""# 7. Extracting high-level features with Stacked Autoencoders

## 7.1. Activation function - GELU (Gaussian Error)

GELU - Gaussian Error Linear Unites was recently proposed - link. In the paper the authors show several instances in which neural networks using GELU outperform networks using ReLU as an activation. gelu is also used in BERT, the NLP approach we used for news sentiment analysis.

We will use GELU for the autoencoders.
"""

def gelu(x):
    return 0.5 * x * (1 + math.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * math.pow(x, 3))))
def relu(x):
    return max(x, 0)
def lrelu(x):
    return max(0.01 * x, x)

"""### Visualize"""

plt.figure(figsize = (15, 5))
plt.subplots_adjust(left = None, bottom = None, right = None, top = None, wspace =.5, hspace = None)

ranges_ = (-10, 3, .25)

plt.subplot(1, 2, 1)
plt.plot([i for i in np.arange(*ranges_)], [relu(i) for i in np.arange(*ranges_)], label = 'ReLU', marker='.')
plt.plot([i for i in np.arange(*ranges_)], [gelu(i) for i in np.arange(*ranges_)], label = 'GELU')
plt.hlines(0, -10, 3, colors='gray', linestyles='--', label='0')
plt.title('Figure 7: GELU as an activation function for autoencoders')
plt.ylabel('f(x) for GELU and ReLU')
plt.xlabel('x')
plt.legend()

plt.subplot(1, 2, 2)
plt.plot([i for i in np.arange(*ranges_)], [lrelu(i) for i in np.arange(*ranges_)], label = 'Leaky ReLU')
plt.hlines(0, -10, 3, colors='gray', linestyles='--', label = '0')
plt.ylabel('f(x) for Leaky ReLU')
plt.xlabel('x')
plt.title('Figure 8: LeakyReLU')
plt.legend()

plt.show()

"""Normally, in autoencoders the number of encoders == number of decoders. We want, however, to extract higher level features (rather than creating the same input), so we can skip the last layer in the decoder. We achieve this creating the encoder and decoder with same number of layers during the training, but when we create the output we use the layer next to the only one as it would contain the higher level features."""

batch_size = 64
n_batches = VAE_data.shape[0]/batch_size
num_training_days = 1495

VAEdata = VAE_data.copy()

data_train = VAEdata.iloc[:num_training_days, [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26]]
data_train = data_train.values
label_train = VAEdata.iloc[:num_training_days, 5]
label_train = label_train.values

data_test = VAEdata.iloc[num_training_days:, [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 21, 22, 23, 24, 26]]
data_test = data_test.values
label_test = VAEdata.iloc[num_training_days:, 5]
label_test = label_test.values

train_iter = mx.io.NDArrayIter(data_train, label_train, batch_size)
test_iter = mx.io.NDArrayIter(data_test, label_test, batch_size)

model_ctx =  mx.cpu()
class VAE(gluon.HybridBlock):
    def __init__(self, n_hidden=400, n_latent=2, n_layers=1, n_output=784, \
                 batch_size=100, act_type='relu', **kwargs):
        self.soft_zero = 1e-10
        self.n_latent = n_latent
        self.batch_size = batch_size
        self.output = None
        self.mu = None
        super(VAE, self).__init__(**kwargs)
        
        with self.name_scope():
            self.encoder = nn.HybridSequential(prefix='encoder')
            
            for i in range(n_layers):
                self.encoder.add(nn.Dense(n_hidden, activation=act_type))
            self.encoder.add(nn.Dense(n_latent*2, activation=None))

            self.decoder = nn.HybridSequential(prefix='decoder')
            for i in range(n_layers):
                self.decoder.add(nn.Dense(n_hidden, activation=act_type))
            self.decoder.add(nn.Dense(n_output, activation='sigmoid'))

    def hybrid_forward(self, F, x):
        h = self.encoder(x)
        #print(h)
        mu_lv = F.split(h, axis=1, num_outputs=2)
        mu = mu_lv[0]
        lv = mu_lv[1]
        self.mu = mu

        eps = F.random_normal(loc=0, scale=1, shape=(self.batch_size, self.n_latent), ctx=model_ctx)
        z = mu + F.exp(0.5*lv)*eps
        y = self.decoder(z)
        self.output = y

        KL = 0.5*F.sum(1+lv-mu*mu-F.exp(lv),axis=1)
        logloss = F.sum(x*F.log(y+self.soft_zero)+ (1-x)*F.log(1-y+self.soft_zero), axis=1)
        loss = -logloss-KL

        return loss

n_hidden = 400 # neurons in each layer
n_latent = 2 
n_layers = 3 # num of dense layers in encoder and decoder respectively
n_output = VAE_data.shape[1]-1 

net = VAE(n_hidden = n_hidden, n_latent = n_latent, n_layers = n_layers, n_output = n_output, batch_size = batch_size, act_type = 'relu')

net.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu())
net.hybridize()
trainer = gluon.Trainer(net.collect_params(), 'adam', {'learning_rate': .01})

print(net)

"""# 8. Generative Adversarial Network (GAN)

A GAN network consists of two models - a Generator ($G$) and Discriminator ($D$). The steps in training a GAN are:

1. The Generator is, using random data (noise denoted $z$), trying to 'generate' data indistinguishable of, or extremely close to, the real data. It's purpose is to learn the distribution of the real data.
2. Randomly, real or generated data is fitted into the Discriminator, which acts as a classifier and tries to understand whether the data is coming from the Generator or is the real data. $D$ estimates the (distributions) probabilities of the incoming sample to the real dataset. (more info on comparing two distributions in section 4.2. below).
3. Then, the losses from $G$ and $D$ are combined and propagated back through the generator. Ergo, the generator's loss depends on both the generator and the discriminator. This is the step that helps the Generator learn about the real data distribution. If the generator doesn't do a good job at generating a realistic data (having the same distribution), the Discriminator's work will be very easy to distinguish generated from real data sets. Hence, the Discriminator's loss will be very small. Small discriminator loss will result in bigger generator loss (see the equation below for $L(D, G)$). This makes creating the discriminator a bit tricky, because too good of a discriminator will always result in a huge generator loss, making the generator unable to learn.
4. The process goes on until the Discriminator can no longer distinguish generated from real data.
      
When combined together, $D$ and $G$ as sort of playing a minmax game (the Generator is trying to fool the Discriminator making it increase the probability for on fake examples, i.e. minimize $\mathbb{E}{z \sim p{z}(z)} [\log (1 - D(G(z)))]$. The Discriminator wants to separate the data coming from the Generator, $D(G(z))$, by maximizing $\mathbb{E}{x \sim p{r}(x)} [\log D(x)]$). Having separated loss functions, however, it is not clear how both can converge together (that is why we use some advancements over the plain GANs, such as Wasserstein GAN). Overall, the combined loss function looks like:

$$L(D, G) = \mathbb{E}{x \sim p{r}(x)} [\log D(x)] + \mathbb{E}_{z \sim p_z(z)} [\log(1 - D(G(z)))]$$

Generative Adversarial Networks (GAN) have been recently used mainly in creating realistic images, paintings, and video clips. There aren't many applications of GANs being used for predicting time-series data as in our case. The main idea, however, should be same - we want to predict future stock movements. In the future, the pattern and behavior of GS's stock should be more or less the same (unless it starts operating in a totally different way, or the economy drastically changes). Hence, we want to 'generate' data for the future that will have similar (not absolutely the same, of course) distribution as the one we already have - the historical trading data.

In our case, we will use LSTM as a time-series generator, and CNN as a discriminator.

## 8.1. The Generator - One layer RNN

### LSTM or GRU

In most cases LSTM and GRU give similar results in terms of accuracy but GRU is much less computational intensive, as GRU has much fewer trainable params. LSTMs, however, and much more used.

Strictly speaking, the math behind the LSTM cell (the gates) is:

$$g_t = \text{tanh}(X_t W_{xg} + h_{t-1} W_{hg} + b_g),$$

$$i_t = \sigma(X_t W_{xi} + h_{t-1} W_{hi} + b_i),$$

$$f_t = \sigma(X_t W_{xf} + h_{t-1} W_{hf} + b_f),$$

$$o_t = \sigma(X_t W_{xo} + h_{t-1} W_{ho} + b_o),$$

$$c_t = f_t \odot c_{t-1} + i_t \odot g_t,$$

$$h_t = o_t \odot \text{tanh}(c_t),$$

where $\odot$ is an element-wise multiplication operator, and, for all $x = [x_1, x_2, \ldots, x_k]^\top \in R^k$ the two activation functions:,

$$\sigma(x) = \left[\frac{1}{1+\exp(-x_1)}, \ldots, \frac{1}{1+\exp(-x_k)}]\right]^\top,$$

$$\text{tanh}(x) = \left[\frac{1-\exp(-2x_1)}{1+\exp(-2x_1)}, \ldots, \frac{1-\exp(-2x_k)}{1+\exp(-2x_k)}\right]^\top$$

### The LSTM architecture
"""

# dataset_total_df (include related assets, need add more data to use it)

gan_num_features = dataset_ex_df.shape[1]
sequence_length = 17

class RNNModel(gluon.Block):
    def __init__(self, num_embed, num_hidden, num_layers, bidirectional=False, \
                 sequence_length=sequence_length, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.num_hidden = num_hidden
        with self.name_scope():
            self.rnn = rnn.LSTM(num_hidden, num_layers, input_size=num_embed, \
                                bidirectional=bidirectional, layout='TNC')
            
            self.decoder = nn.Dense(1, in_units=num_hidden)
    
    def forward(self, inputs, hidden):
        output, hidden = self.rnn(inputs, hidden)
        decoded = self.decoder(output.reshape((-1, self.num_hidden)))
        return decoded, hidden
    
    def begin_state(self, *args, **kwargs):
        return self.rnn.begin_state(*args, **kwargs)
    
lstm_model = RNNModel(num_embed=gan_num_features, num_hidden=500, num_layers=1)
lstm_model.collect_params().initialize(mx.init.Xavier(), ctx=mx.cpu())
trainer = gluon.Trainer(lstm_model.collect_params(), 'adam', {'learning_rate': .01})
loss = gluon.loss.L1Loss()

print(lstm_model)

"""### Learning Rate Scheduler"""

class TriangularSchedule():
    def __init__(self, min_lr, max_lr, cycle_length, inc_fraction=0.5):     
        self.min_lr = min_lr
        self.max_lr = max_lr
        self.cycle_length = cycle_length
        self.inc_fraction = inc_fraction
        
    def __call__(self, iteration):
        if iteration <= self.cycle_length*self.inc_fraction:
            unit_cycle = iteration * 1 / (self.cycle_length * self.inc_fraction)
        elif iteration <= self.cycle_length:
            unit_cycle = (self.cycle_length - iteration) * 1 / (self.cycle_length * (1 - self.inc_fraction))
        else:
            unit_cycle = 0
        adjusted_cycle = (unit_cycle * (self.max_lr - self.min_lr)) + self.min_lr
        return adjusted_cycle

class CyclicalSchedule():
    def __init__(self, schedule_class, cycle_length, cycle_length_decay=1, cycle_magnitude_decay=1, **kwargs):
        self.schedule_class = schedule_class
        self.length = cycle_length
        self.length_decay = cycle_length_decay
        self.magnitude_decay = cycle_magnitude_decay
        self.kwargs = kwargs
    
    def __call__(self, iteration):
        cycle_idx = 0
        cycle_length = self.length
        idx = self.length
        while idx <= iteration:
            cycle_length = math.ceil(cycle_length * self.length_decay)
            cycle_idx += 1
            idx += cycle_length
        cycle_offset = iteration - idx + cycle_length
        
        schedule = self.schedule_class(cycle_length=cycle_length, **self.kwargs)
        return schedule(cycle_offset) * self.magnitude_decay**cycle_idx

schedule = CyclicalSchedule(TriangularSchedule, min_lr=0.5, max_lr=2, cycle_length=500)
iterations=1500

plt.plot([i+1 for i in range(iterations)],[schedule(i) for i in range(iterations)])
plt.title('Learning rate for each epoch')
plt.xlabel("Epoch")
plt.ylabel("Learning Rate")
plt.show()

"""## 8.2. The Discriminator - One Dimentional CNN

We usually use CNNs for work related to images (classification, context extraction, etc). They are very powerful at extracting features from features from features, etc. For example, in an image of a dog, the first convolutional layer will detect edges, the second will start detecting circles, and the third will detect a nose. In our case, data points form small trends, small trends form bigger, trends in turn form patterns. CNNs' ability to detect features can be used for extracting information about patterns in GS's stock price movements.

Another reason for using CNN is that CNNs work well on spatial data - meaning data points that are closer to each other are more related to each other, than data points spread across. This should hold true for time series data. In our case each data point (for each feature) is for each consecutive day. It is natural to assume that the closer two days are to each other, the more related they are to each other. One thing to consider (although not covered in this work) is seasonality and how it might change (if at all) the work of the CNN.

# 9. Hyperparameters Optimization

After the GAN trains on the 200 epochs it will record the MAE (which is the error function in the LSTM, the $G$) and pass it as a reward value to the Reinforcement Learning (RL) that will decide whether to change the hyperparameters of keep training with the same set of hyperparameters. As described later, this approach is strictly for experimenting with RL.

If the RL decides it will update the hyperparameters it will call Bayesian optimisation (discussed below) library that will give the next best expected set of the hyperparams.

## Reinforcement Learning for Hyperparameters Optimization

## Bayesian Optimization

Instead of the grid search, that can take a lot of time to find the best combination of hyperparameters, we will use Bayesian optimization. The library that we'll use is already implemented - https://github.com/fmfn/BayesianOptimization
"""