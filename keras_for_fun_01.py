# -*- coding: utf-8 -*-
"""Keras for Fun_01.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/12TCBOw3pPruzgZu_-IZ4NUhCrqJjDwcZ

# Getting started with the Keras Sequential model

The Sequential model is a linear stack of layers.\
You can create a Sequential model by passing a list of layer instances to the constructor:
"""

import keras

from keras.models import Sequential
from keras.layers import Dense, Activation

model = Sequential([
    Dense(32, input_shape = (784,)),
    Activation('relu'),
    Dense(10),
    Activation('softmax'),
])

"""You can also simply add layers via the .add() method:"""

model = Sequential()
model.add(Dense(32, input_dim = 784))
model.add(Activation('relu'))

"""## Specifying the input shape

The model needs to know what input shape it should expect.\
For this reason, **the first layer in a Sequential model** (and only the first, because ***following layers can do automatic shape inference***) **needs to receive information about its input shape**. There are several possible ways to do this:
*  Pass an **input_shape argument** to the first layer. This is a shape tuple (a tuple of integers or None entries, where None indicates that any positive integer may be expected). *In input_shape, the batch dimension is not included*.
*  Some **2D layers**, such as **Dense**, support the specification of their input shape via the **argument input_dim**, and some **3D temporal layers** support the **arguments input_dim** and **input_length**.
*  If you ever need to specify a fixed batch size for your inputs (this is useful for stateful recurrent networks), you can pass a **batch_size argument** to a layer. If you *pass both batch_size = 32 and input_shape = (6, 8) to a layer*, it will then *expect every batch of inputs to have the batch shape (32, 6, 8)*.

As such, the following snippets are strictly equivalent:
"""

model = Sequential()
model.add(Dense(32, input_shape = (784,)))

# or

model = Sequential()
model.add(Dense(32, input_dim = 784)) # Batch_size = 32

"""## Compilation

Before training a model, you need to configure the learning process, which is done via the compile method.\
It receives three arguments:

* An **optimizer**. This could be the string identifier of an existing optimizer (such as rmsprop or adagrad), or an instance of the Optimizer class.
* A **loss function**. This is the objective that the model will try to minimize. It can be the string identifier of an existing loss function (such as categorical_crossentropy or mse), or it can be an objective function.
* A **list of metrics**. For any classification problem you will want to set this to metrics=['accuracy']. A metric could be the string identifier of an existing metric or a custom metric function.
"""

# For a multi-class classification problem
model.compile(optimizer = 'rmsprop',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

# For a binary classification problem
model.compile(optimizer = 'rmsprop',
              loss = 'binary_crossentropy',
              metrics=['accuracy'])

# For a mean squared error regression problem
model.compile(optimizer = 'rmsprop',
              loss = 'mse')

# For custom metrics
import keras.backend as K

def mean_pred(y_true, y_pred):
    return K.mean(y_pred)

model.compile(optimizer = 'rmsprop',
              loss = 'binary_crossentropy',
              metrics = ['accuracy', mean_pred])

"""# Training

Keras models are trained on Numpy arrays of input data and labels.\
For training a model, you will typically use the fit function.\
Trains the model for a given number of epochs (iterations on a dataset).
"""

# For a single-input model with 2 classes (binary classification):

model = Sequential()
model.add(Dense(32, activation = 'relu', input_dim = 100))
model.add(Dense(1, activation = 'sigmoid'))
model.compile(optimizer = 'rmsprop',
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])

# Generate dummy data
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(2, size = (1000, 1))

# Train the model, iterating on the data in batches of 32 samples. Setup 10 epoches
model.fit(data, labels, epochs = 10, batch_size = 32)

data.shape, labels.shape # Data and label have a same number of rows

# For a single-input model with 10 classes (categorical classification):

model = Sequential()
model.add(Dense(32, activation = 'relu', input_dim = 100))
model.add(Dense(10, activation = 'softmax'))
model.compile(optimizer = 'rmsprop',
              loss = 'categorical_crossentropy',
              metrics = ['accuracy'])

# Generate dummy data
import numpy as np
data = np.random.random((1000, 100))
labels = np.random.randint(10, size = (1000, 1))

# Convert labels to categorical one-hot encoding
one_hot_labels = keras.utils.to_categorical(labels, num_classes = 10)

# Train the model, iterating on the data in batches of 32 samples
model.fit(data, one_hot_labels, epochs = 10, batch_size = 32)

"""# Example

In the examples folder, you will also find example models for real datasets:

* CIFAR10 small images classification: Convolutional Neural Network (CNN) with realtime data augmentation
* IMDB movie review sentiment classification: LSTM over sequences of words
* Reuters newswires topic classification: Multilayer Perceptron (MLP)
* MNIST handwritten digits classification: MLP & CNN
* Character-level text generation with LSTM

## Multilayer Perceptron (MLP) for multi-class softmax classification

Softmax - Normalized Exponential Function.\
 Softmax is often used in neural networks, to map the non-normalized output of a network to a probability distribution over predicted output classes.
"""

import keras
from keras.models import Sequential
from keras.layers import Dense, Dropout, Activation
from keras.optimizers import SGD

# Generate dummy data

# import numpy as np
x_train = np.random.random((1000, 20))
y_train = keras.utils.to_categorical(np.random.randint(10, size = (1000, 1)), num_classes = 10)
x_test = np.random.random((100, 20))
y_test = keras.utils.to_categorical(np.random.randint(10, size = (100, 1)), num_classes = 10)

x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_train[0], x_train[0][0], y_train[0], y_train[0][0], y_train[1:10]

# y is one hot vector, as class, label

model = Sequential()

# Dense(64) is a fully-connected layer with 64 hidden units.
# in the first layer, you must specify the expected input data shape:
# here, 20-dimensional vectors.

model.add(Dense(64, activation = 'relu', input_dim = 20))
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))

model.add(Dropout(0.5))
model.add(Dense(10, activation = 'softmax'))

sgd = SGD(lr = 0.01, decay = 1e-6, momentum = 0.9, nesterov = True)
model.compile(loss = 'categorical_crossentropy',
              optimizer = sgd,
              metrics = ['accuracy'])

model_fit = model.fit(x_train, y_train,
          epochs = 20,
          batch_size = 128)

score = model.evaluate(x_test, y_test, batch_size = 128)

# model_fit.epoch, model_fit.history, model_fit.on_batch_begin, model_fit.on_batch_end, model_fit.on_epoch_begin, model_fit.on_epoch_end, model_fit.on_train_begin, model_fit.on_train_end, model_fit.params, model_fit.set_model, model_fit.set_params, model_fit.validation_data

score

"""## MLP for binary classification"""

# import numpy as np
# from keras.models import Sequential
# from keras.layers import Dense, Dropout

# Generate dummy data
x_train = np.random.random((1000, 20))
y_train = np.random.randint(2, size = (1000, 1))
x_test = np.random.random((100, 20))
y_test = np.random.randint(2, size = (100, 1))

model = Sequential()
model.add(Dense(64, input_dim = 20, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(64, activation = 'relu'))
model.add(Dropout(0.5))
model.add(Dense(1, activation = 'sigmoid'))

model.compile(loss = 'binary_crossentropy',
              optimizer = 'rmsprop',
              metrics=['accuracy'])

model.fit(x_train, y_train,
          epochs = 20,
          batch_size = 128)

score = model.evaluate(x_test, y_test, batch_size = 128)

x_train.shape, y_train.shape, x_test.shape, y_test.shape, x_train[0], x_train[0][0], y_train[0], y_train[0][0], y_train[1:10]

score